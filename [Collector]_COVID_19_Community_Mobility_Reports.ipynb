{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Collector] - COVID-19 Community Mobility Reports.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-v9_LsWaGQa",
        "colab_type": "text"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "de1e2863-bf85-41fb-92c9-2d04ce87762c",
        "id": "bLFSAHhEXjCt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "!pip install pdfminer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdfminer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/a3/155c5cde5f9c0b1069043b2946a93f54a41fd72cc19c6c100f6f2f5bdc15/pdfminer-20191125.tar.gz (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 2.5MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/16/da16a22d47bac9bf9db39f3b9af74e8eeed8855c0df96be20b580ef92fff/pycryptodome-3.9.7-cp36-cp36m-manylinux1_x86_64.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 46.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
            "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer: filename=pdfminer-20191125-cp36-none-any.whl size=6140077 sha256=bda9db467b974352dd796d7d4bc18083f1e2def355865a9ef723a8e2b36b4077\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/00/af/720a55d74ba3615bb4709a3ded6dd71dc5370a586a0ff6f326\n",
            "Successfully built pdfminer\n",
            "Installing collected packages: pycryptodome, pdfminer\n",
            "Successfully installed pdfminer-20191125 pycryptodome-3.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqCcPl0rj8SQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "2f8f5b6e-9acc-4f7a-b244-595ec6eeefc2"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=a82db949a262a065d3a25935ffb5a88aec22e6e6b1f9cf1a0782caa57070374a\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ylsXuPaJqQ",
        "colab_type": "text"
      },
      "source": [
        "## PDF Mining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hSWPMrJyXjCw",
        "colab": {}
      },
      "source": [
        "# reference: https://dzone.com/articles/exporting-data-from-pdfs-with-python\n",
        "\n",
        "import io\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.pdfinterp import PDFResourceManager\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "import re\n",
        "\n",
        "\n",
        "def extract_text_by_page(pdf_path):\n",
        "    with open(pdf_path, 'rb') as fh:\n",
        "        for page in PDFPage.get_pages(fh, \n",
        "                                      caching=True,\n",
        "                                      check_extractable=True):\n",
        "            resource_manager = PDFResourceManager()\n",
        "            fake_file_handle = io.StringIO()\n",
        "            converter = TextConverter(resource_manager, fake_file_handle)\n",
        "            page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
        "            page_interpreter.process_page(page)\n",
        "            text = fake_file_handle.getvalue()\n",
        "            yield text\n",
        "            # close open handles\n",
        "            converter.close()\n",
        "            fake_file_handle.close()\n",
        "def extract_text(pdf_path):\n",
        "\n",
        "    country = None\n",
        "    state = None\n",
        "    date = None\n",
        "    data = {}\n",
        "    # only for countries\n",
        "    features = []\n",
        "\n",
        "    for i,page in enumerate(extract_text_by_page(pdf_path)):\n",
        "        #print(page)\n",
        "        # page 1 and 2 is related to country\n",
        "        if (i == 0 or i == 1):\n",
        "          match = re.search(r'COVID-19 Community Mobility Report(.*) (\\S+ \\d+, \\d+)', page)\n",
        "          if (match):\n",
        "            country = match.group(1)\n",
        "            date = match.group(2)\n",
        "            #print(country,date)\n",
        "          match_features = re.search(r'Retail & recreation(\\S+)(%.+Grocery & pharmacy)(\\S+)(%.+Parks)(\\S+)%', page)\n",
        "          if (match_features):\n",
        "            features.append(match_features.group(1))\n",
        "            features.append(match_features.group(3))\n",
        "            features.append(match_features.group(5))\n",
        "          match_features = re.search(r'Transit stations(\\S+)(%.+Workplaces)(\\S+)(%.+Residential)(\\S+)%', page)\n",
        "          if (match_features):\n",
        "            features.append(match_features.group(1))\n",
        "            features.append(match_features.group(3))\n",
        "            features.append(match_features.group(5))\n",
        "            data[country] = features.copy()\n",
        "            features.clear()\n",
        "        # some states/departments/county of other countries have the string \"Not enough data for this date\"\n",
        "        # that make the rows to bug some details of information, thus, only Brazil states are collect    \n",
        "        elif country == \"Brazil\":\n",
        "          # first state/department in the page    \n",
        "          first = re.search(r'(.{5,30})Retail & recreation(\\S+)(%.{15,25}Grocery & pharmacy)(\\S+)(%.{15,25}Parks)(\\S+)(%.{15,25}Transit stations)(\\S+)(%.{15,25}Workplace)(\\S+)(%.{15,25}Residential)(\\S+)%', page)\n",
        "          # second state/department in the page\n",
        "          second = re.search(r'baseline(.{5,30})Retail & recreation(\\S+)(%.{15,25}Grocery & pharmacy)(\\S+)(%.{15,25}Parks)(\\S+)(%.{15,25}Transit stations)(\\S+)(%.{15,25}Workplace)(\\S+)(%.{15,25}Residential)(\\S+)%', page)\n",
        "\n",
        "          if first:\n",
        "            data[first.group(1)] = [first.group(2),first.group(4),first.group(6),first.group(8),first.group(10),first.group(12)]\n",
        "          if second:\n",
        "            data[second.group(1)] = [second.group(2),second.group(4),second.group(6),second.group(8),second.group(10),second.group(12)]\n",
        "    return country, date, data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSn3W4xwaaBM",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvPQF48UpXW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_links = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urfE2-ULlrWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import package\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# specify the url\n",
        "url = 'https://www.google.com/covid19/mobility/'\n",
        "\n",
        "# packages the request, send the request and catch the response\n",
        "response = requests.get(url)\n",
        "\n",
        "# extract the content\n",
        "content = response.content\n",
        "\n",
        "parser = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "# Get a list of all links.\n",
        "a_tags = parser.find_all(\"a\")\n",
        "# Get the text\n",
        "for link in a_tags:\n",
        "  link_str = link.get('href')   \n",
        "  # this was necessary because google list american county/state in separate files (lets eliminate them)\n",
        "  ispdf = re.search(r'(.*)_(.{2})_Mobility(.*)pdf',link_str)\n",
        "  if ispdf:\n",
        "    all_links.append(ispdf.group(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhKdGK8ypgTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_links"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjTsX7LwqCxt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "23293357-dfd1-46b7-d088-e8913c306aa9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# Final Dataframe\n",
        "df_final = pd.DataFrame()\n",
        "\n",
        "time1 = time.time()\n",
        "print('Collect begins at {}'.format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
        "for i,link in enumerate(all_links):\n",
        "  # Create a pdf file from the link\n",
        "  pdfname = str(i)+\".pdf\" \n",
        "  open(pdfname, 'wb').write(requests.get(link).content)\n",
        "\n",
        "  # Extract information\n",
        "  country, date, data = extract_text(pdfname)\n",
        "\n",
        "  # Steps to create dataframe\n",
        "  # columns names\n",
        "  columns = [\"Retail & recreation\",\"Grocery & pharmacy\",\"Parks\",\"Transit stations\",\"Workplace\",\"Residential\"]\n",
        "  \n",
        "  # create dataframe\n",
        "  df = pd.DataFrame.from_dict(data, orient='index', columns=columns)\n",
        "\n",
        "  # convert all data to int32\n",
        "  for col in columns:\n",
        "    df[col] = df[col].astype(np.int32)\n",
        "\n",
        "  # create column to identify the country\n",
        "  df[\"Country\"] = country\n",
        "\n",
        "  # Concatenate temporary dataframe for the final ones\n",
        "  df_final = pd.concat([df_final, df])\n",
        "\n",
        "print('Collect ends at {}'.format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
        "print('Duration: {}s'.format(time.time() - time1))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collect begins at 2020-04-04 17:42:05\n",
            "Collect ends at 2020-04-04 17:44:10\n",
            "Duration: 125.05038547515869s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-Ug7thasMCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "2c3bbb4a-47a4-48ec-8fa3-03413d8be797"
      },
      "source": [
        "df_final.head(5)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Retail &amp; recreation</th>\n",
              "      <th>Grocery &amp; pharmacy</th>\n",
              "      <th>Parks</th>\n",
              "      <th>Transit stations</th>\n",
              "      <th>Workplace</th>\n",
              "      <th>Residential</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Afghanistan</th>\n",
              "      <td>-38</td>\n",
              "      <td>-21</td>\n",
              "      <td>-13</td>\n",
              "      <td>-34</td>\n",
              "      <td>-33</td>\n",
              "      <td>10</td>\n",
              "      <td>Afghanistan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Angola</th>\n",
              "      <td>-61</td>\n",
              "      <td>-40</td>\n",
              "      <td>-39</td>\n",
              "      <td>-57</td>\n",
              "      <td>-11</td>\n",
              "      <td>22</td>\n",
              "      <td>Angola</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Argentina</th>\n",
              "      <td>-86</td>\n",
              "      <td>-61</td>\n",
              "      <td>-89</td>\n",
              "      <td>-80</td>\n",
              "      <td>-57</td>\n",
              "      <td>27</td>\n",
              "      <td>Argentina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aruba</th>\n",
              "      <td>-88</td>\n",
              "      <td>-66</td>\n",
              "      <td>-80</td>\n",
              "      <td>-88</td>\n",
              "      <td>-72</td>\n",
              "      <td>20</td>\n",
              "      <td>Aruba</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Australia</th>\n",
              "      <td>-45</td>\n",
              "      <td>-19</td>\n",
              "      <td>-35</td>\n",
              "      <td>-58</td>\n",
              "      <td>-33</td>\n",
              "      <td>13</td>\n",
              "      <td>Australia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Retail & recreation  Grocery & pharmacy  ...  Residential      Country\n",
              "Afghanistan                  -38                 -21  ...           10  Afghanistan\n",
              "Angola                       -61                 -40  ...           22       Angola\n",
              "Argentina                    -86                 -61  ...           27    Argentina\n",
              "Aruba                        -88                 -66  ...           20        Aruba\n",
              "Australia                    -45                 -19  ...           13    Australia\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHZYPAPSzwEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "828c3af7-94d7-448b-cbe3-e8830c824853"
      },
      "source": [
        "print(date)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "March 29, 2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hsz8y_dV1im",
        "colab_type": "text"
      },
      "source": [
        "## Generate the final dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9lALLPyS_MB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_final.to_csv(date + \".csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}